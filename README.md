##### **Architecture Flow**



This project implements a serverless data processing pipeline on AWS designed to ingest, transform, and analyze high-volume clickstream events generated by an e-commerce platform.



End-to-end flow:



Producer

→ Amazon Kinesis Data Firehose (stream ingestion)

→ Amazon S3 (Raw Zone) – JSON data partitioned by year/month/day

→ AWS Glue ETL Job – transforms JSON to optimized Parquet format

→ Amazon S3 (Processed Zone) – partitioned Parquet data

→ AWS Glue Crawler – automatically updates the Data Catalog

→ Amazon Athena – SQL-based analytics on curated data



This architecture is fully serverless, auto-scalable, and cost-efficient.

Kinesis Data Firehose: Fully managed streaming ingestion service that automatically scales and delivers data to S3 without managing consumers.



Amazon S3: Durable, low-cost storage for both raw and processed data layers.



AWS Glue: Serverless Spark-based ETL service used to clean, transform, and optimize data.



Parquet Format: Columnar storage format that significantly reduces Athena query cost and latency.



AWS Glue Crawler: Automatically infers schema and updates metadata for Athena.



Amazon Athena: Serverless SQL engine for querying data directly from S3.



Terraform: Infrastructure as Code tool used to provision and manage all cloud resources consistently.



##### **Deployment Instructions (Infrastructure as Code)**



All AWS resources are provisioned using Terraform.



###### Prerequisites



AWS CLI configured with valid credentials



Terraform installed



An AWS account



###### Steps to Deploy Infrastructure

cd iac

terraform init

terraform apply





This will create:



S3 buckets for raw and processed data



IAM roles for Glue and Firehose



Kinesis Data Firehose delivery stream



AWS Glue ETL job



AWS Glue Crawler



Glue Data Catalog database



##### **Data Transformation (AWS Glue)**



The Glue ETL job performs the following steps:



Reads raw JSON clickstream data from the S3 raw bucket



Selects required fields (user\_id, event\_type, product\_id, timestamp)



Adds partition columns (year, month, day) derived from the timestamp



Writes the transformed data in Parquet format



Stores data in a partitioned structure optimized for Athena queries



This approach improves query performance and reduces scan costs.



##### **Sample Data**



A sample clickstream event is provided to demonstrate the expected data structure:



sample\_data/clickstream.json





Example fields include:



user\_id



event\_type



product\_id



timestamp



In a production scenario, these events are continuously ingested through Kinesis Data Firehose.



##### **Metadata Cataloging**



An AWS Glue Crawler scans the processed S3 Parquet data and updates the Glue Data Catalog automatically.

This allows Amazon Athena to immediately query new data without manual table creation.



##### **Querying with Amazon Athena**



Sample SQL queries are provided in:



athena/sample\_queries.sql





Example use cases:



Count events by event type



Analyze user behavior patterns



Generate analytics for business intelligence



Athena queries the partitioned Parquet data directly from S3, providing fast and cost-efficient analytics.



##### **Security and Monitoring**



IAM roles are configured using least-privilege access



Data stored in S3 can be encrypted at rest



AWS Glue and Firehose logs are available in Amazon CloudWatch



Serverless services eliminate infrastructure management overhead



##### **AI Tooling Disclosure**



ChatGPT was used to assist with:



Terraform structure and syntax validation



Glue ETL script formatting



Documentation organization and clarity



All architectural decisions, configurations, and final implementations were reviewed and validated manually.



##### **Summary**



This project demonstrates a production-style, serverless data engineering pipeline on AWS using modern best practices, Infrastructure as Code, and managed analytics services. It reflects real-world architectures commonly implemented in enterprise cloud environments.

