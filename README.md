Serverless Data Processing Pipeline (ETL / ELT)

Overview



This project implements a production-style, serverless data processing pipeline on AWS for ingesting, transforming, and analyzing high-volume clickstream events generated by an e-commerce platform.



The solution leverages fully managed AWS services and Infrastructure as Code (Terraform) to ensure scalability, reliability, cost efficiency, and repeatable deployments. The architecture reflects real-world cloud data engineering patterns commonly used in enterprise environments.



Architecture Flow



End-to-end data flow:



Producer

→ Amazon Kinesis Data Firehose (stream ingestion)

→ Amazon S3 – Raw Zone (JSON data, date-partitioned)

→ AWS Glue ETL Job (JSON → Parquet transformation)

→ Amazon S3 – Processed Zone (Parquet, partitioned by year/month/day)

→ AWS Glue Crawler (automatic schema discovery)

→ Amazon Athena (SQL analytics)



This architecture is fully serverless, auto-scales with workload demand, and requires no infrastructure management.



Why These AWS Services Were Chosen



Kinesis Data Firehose

Provides fully managed, scalable ingestion of streaming data without managing producers or consumers.



Amazon S3

Durable and cost-effective storage for both raw and curated datasets.



AWS Glue

Serverless Spark-based ETL service for large-scale data transformation and schema evolution.



Parquet Format

Columnar storage format optimized for analytics, significantly reducing Athena query cost and latency.



AWS Glue Crawler

Automatically maintains the Glue Data Catalog, eliminating manual table management.



Amazon Athena

Serverless SQL query engine enabling fast analytics directly on S3 data.



Terraform

Infrastructure as Code tool used to provision and manage all cloud resources consistently and reproducibly.



Deployment Instructions (Infrastructure as Code)



All cloud resources are provisioned using Terraform.



Prerequisites



AWS account



AWS CLI configured with valid credentials



Terraform installed locally



Steps to Deploy

cd iac

terraform init

terraform apply





This deployment creates:



S3 buckets for raw and processed data



IAM roles for Glue and Firehose



Kinesis Data Firehose delivery stream



AWS Glue ETL job



AWS Glue Crawler



Glue Data Catalog database



Data Transformation (AWS Glue)



The AWS Glue ETL job performs the following steps:



Reads raw JSON clickstream data from the S3 raw zone



Selects relevant business fields



Adds partition columns (year, month, day) derived from event timestamps



Writes optimized partitioned Parquet files to the processed S3 zone



Partitioning significantly improves Athena query performance and reduces scan costs.



Sample Data



A sample clickstream event is included to demonstrate the expected input schema:



sample\_data/clickstream.json





In a production environment, this data would be continuously ingested via Kinesis Data Firehose.



Querying with Amazon Athena



Sample SQL queries are available in:



athena/sample\_queries.sql





These queries demonstrate how analysts can:



Analyze event trends



Aggregate user behavior



Generate insights for business intelligence use cases





\## Monitoring and Observability



\- AWS Glue job logs and metrics are available in Amazon CloudWatch

\- Kinesis Firehose delivery success and failure metrics can be monitored via CloudWatch

\- Failed Firehose records can be routed to S3 error prefixes for investigation

\- These metrics enable alerting, operational visibility, and faster incident resolution



\## Production Improvements (Future Enhancements)



If deployed in a production environment, the following improvements would be implemented:

\- Dead-letter queues (DLQ) for failed Firehose records

\- Data quality checks using AWS Glue Data Quality or Great Expectations

\- CI/CD pipelines for Terraform using GitHub Actions

\- Fine-grained IAM permissions and restrictive S3 bucket policies

\- Cost monitoring using AWS Cost Explorer and Athena query metrics



\## Results



\- Fully serverless architecture with zero infrastructure management

\- Scalable ingestion capable of handling high-volume clickstream events

\- Optimized analytics using partitioned Parquet and Amazon Athena

\- Reproducible environments using Infrastructure as Code (Terraform)



AI Tooling Disclosure



ChatGPT was used to assist with:



Terraform structure and syntax validation



AWS Glue ETL script formatting



Documentation organization and clarity



All architectural decisions, configurations, and final implementations were reviewed and validated manually.



Summary



This project demonstrates end-to-end ownership of a cloud-native data pipeline, combining scalable ingestion, optimized transformations, automated metadata management, and serverless analytics. It reflects the design patterns and best practices used in modern enterprise data engineering environments.

