\# Serverless Data Processing Pipeline (ETL/ELT)



\## Overview



This project shows how to build a fully serverless data pipeline on AWS to handle large volumes of clickstream data generated by an e-commerce application. The pipeline ingests raw data, processes and transforms it, and makes it available for analytics in a scalable and cost-efficient way.

\## Architecture Flow

Producer → Kinesis Data Firehose → S3 (Raw, Date Partitioned)

→ AWS Glue ETL → S3 (Parquet, Partitioned)

→ Glue Crawler → Athena



\## Key Design Decisions

\- Firehose for fully managed streaming ingestion

\- Parquet + partitioning for Athena performance

\- Glue Crawler for automatic schema management

\- Terraform for repeatable infrastructure



\## Monitoring \& Security

\- IAM roles with least privilege

\- S3 encryption at rest

\- CloudWatch logging for Glue jobs





\## Architecture



The solution uses the following AWS services:

• Amazon Kinesis Data Firehose to ingest streaming clickstream events in real time

• Amazon S3 to store raw and processed data using date-based partitions

• AWS Glue to transform raw JSON data into analytics-ready formats

• AWS Glue Crawler to automatically detect schemas and update the data catalog

•Amazon Athena to run SQL queries directly on the processed data in S3

•Terraform to provision and manage all infrastructure as code



\## Deployment



cd iac

terraform init

terraform apply



Once completed, the full serverless pipeline will be ready to use.

